{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import linregress\n",
    "import datetime\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, concatenate, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Tag0': 352461,\n",
       "         'Tag1': 272234,\n",
       "         'Tag2': 377420,\n",
       "         'Tag3': 398370,\n",
       "         'Tag4': 428642,\n",
       "         'Tag5': 435086,\n",
       "         'Tag6': 358288,\n",
       "         'Tag7': 315266,\n",
       "         'Tag8': 431390,\n",
       "         'Tag9': 381880})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_data = pd.DataFrame.from_csv(\"../../data/database/sensor_data.csv\")\n",
    "sensor_data = sensor_data[~((sensor_data.TagName == 'Start1') | (sensor_data.TagName == 'Start2'))]\n",
    "Counter(sensor_data.TagName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing sensortype: 1\n",
      "processing sensortype: 2\n",
      "processing sensortype: 3\n",
      "processing sensortype: 4\n",
      "processing sensortype: 9\n",
      "processing sensortype: 10\n",
      "processing sensortype: 11\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalization(feature, mini = None, maxi = None):\n",
    "    if ((maxi is None) or (mini is None)):\n",
    "        maxi = np.max(feature)\n",
    "        mini = np.min(feature)\n",
    "#         print(maxi)\n",
    "#         print(mini)\n",
    "    else:\n",
    "        if (maxi == mini):\n",
    "            # all the values is same:\n",
    "            return np.array([1] * feature.size).reshape(feature.shape)\n",
    "        if type(feature) == list:\n",
    "            feature = np.array(feature)\n",
    "        feature[feature < mini] = mini\n",
    "        feature[feature > maxi] = maxi\n",
    "\n",
    "    feature = ((feature - mini) / (maxi - mini))\n",
    "        \n",
    "    \n",
    "    return feature\n",
    "\n",
    "if os.path.exists(\"../../data/database/normalized_sensor_data.csv\") is False:\n",
    "    \n",
    "    percentile_df = pd.DataFrame.from_csv(\"../../Results/percentiles_sensortype.txt\")\n",
    "\n",
    "    normalized_sensor_data = pd.DataFrame(columns=sensor_data.columns,index=sensor_data.index)\n",
    "    normalized_sensor_data.loc[:,'SENSORTYPE'] = sensor_data['SENSORTYPE'].values\n",
    "    normalized_sensor_data.loc[:,'TagName'] = sensor_data['TagName'].values\n",
    "    normalized_sensor_data.loc[:,'tester_id'] = sensor_data['tester_id'].values\n",
    "    normalized_sensor_data.loc[:,'TIMESTAMP'] = sensor_data['TIMESTAMP'].values\n",
    "#     for i in range(len(percentile_df)):\n",
    "#         cur = percentile_df.iloc[i].values\n",
    "#         sensor = cur[0]\n",
    "#         val = ['VALUES1','VALUES2', 'VALUES3']\n",
    "#         p97 = cur[1]\n",
    "#         p03 = cur[2]\n",
    "#         normalized_sensor_data.loc[(sensor_data.SENSORTYPE == sensor), val] = min_max_normalization(sensor_data[(sensor_data.SENSORTYPE == sensor)][val].values, p03, p97)\n",
    "#         print(\"processing: \" + str(cur))\n",
    "    for sensor in (percentile_df[' SENSORTYPE'].values):\n",
    "        if sensor == 26:\n",
    "            continue\n",
    "        val = ['VALUES1','VALUES2', 'VALUES3']\n",
    "        print(\"processing sensortype: \" + str(sensor))\n",
    "\n",
    "        data = sensor_data[sensor_data.SENSORTYPE == sensor][val].values.reshape(-1,1)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data)\n",
    "        normalized_sensor_data.loc[(sensor_data.SENSORTYPE == sensor), val] = scaler.transform(data).reshape(int(data.size / 3),3)\n",
    "     \n",
    "#     normalized_sensor_data.to_csv(\"../../data/database/normalized_sensor_data.csv\")\n",
    "#     del sensor_data\n",
    "else:\n",
    "    normalized_sensor_data = pd.DataFrame.from_csv(\"../../data/database/normalized_sensor_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor_data_option = \"Normalize\"\n",
    "if sensor_data_option == \"Normalize\":\n",
    "    # tag_id_groupby = sensor_data_acc_tag12.groupby(['TagName', 'tester_id'])\n",
    "    tag_id_groupby_acc = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 1)].groupby(['TagName', 'tester_id'])\n",
    "    tag_id_dict = tag_id_groupby_acc.groups\n",
    "    y = list(tag_id_dict.keys())\n",
    "    y = [i[0] for i in y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    tag_id_groupby_magnetic = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 2)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_orientation = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 3)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_gyro = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 4)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_gravity = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 9)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_quaternion = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 11)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_tilt = normalized_sensor_data[(normalized_sensor_data.SENSORTYPE == 26)].groupby(['TagName', 'tester_id'])\n",
    "else:\n",
    "\n",
    "    # tag_id_groupby = sensor_data_acc_tag12.groupby(['TagName', 'tester_id'])\n",
    "    tag_id_groupby_acc = sensor_data[(sensor_data.SENSORTYPE == 1)].groupby(['TagName', 'tester_id'])\n",
    "    tag_id_dict = tag_id_groupby_acc.groups\n",
    "    y = list(tag_id_dict.keys())\n",
    "    y = [i[0] for i in y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    tag_id_groupby_magnetic = sensor_data[(sensor_data.SENSORTYPE == 2)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_orientation = sensor_data[(sensor_data.SENSORTYPE == 3)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_gyro = sensor_data[(sensor_data.SENSORTYPE == 4)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_gravity = sensor_data[(sensor_data.SENSORTYPE == 9)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_quaternion = sensor_data[(sensor_data.SENSORTYPE == 11)].groupby(['TagName', 'tester_id'])\n",
    "\n",
    "    tag_id_groupby_tilt = sensor_data[(sensor_data.SENSORTYPE == 26)].groupby(['TagName', 'tester_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gesture_features(accs):\n",
    "#     for i in range(accs.shape[0]): # x, y, z\n",
    "#         accs[i] = min_max_normalization(accs[i], np.min(accs[i]), np.max(accs[i]))\n",
    "        \n",
    "    if N_frame_no > 1:\n",
    "        Ls = math.floor(len(accs)/ (N_frame_no + 1))\n",
    "        segments = None\n",
    "        for i in range(N_frame_no + 1):\n",
    "            if segments is None:\n",
    "                segments = np.array([accs[i*Ls:(i+1)*Ls]])\n",
    "            else:\n",
    "                segments = np.append(segments, np.array([accs[i*Ls:(i+1)*Ls]]), axis=0)\n",
    "\n",
    "        frames = None\n",
    "        for i in range(N_frame_no):\n",
    "            cur_frame = segments[i:i+2]\n",
    "            cur_frame = cur_frame.reshape((cur_frame.shape[0]*cur_frame.shape[1],cur_frame.shape[2]))\n",
    "            if frames is None:\n",
    "                frames = np.array([cur_frame])\n",
    "            else:\n",
    "                frames = np.append(frames, np.array([cur_frame]), axis = 0)\n",
    "        return np.array([frame_features(f) for f in frames]).reshape(-1)\n",
    "    else:\n",
    "        return frame_features(accs).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frame_features(cur_frame):\n",
    "    dft_cur_frame = np.fft.fftn(cur_frame)\n",
    "    \n",
    "    mean_cur_frame = dft_cur_frame[0]\n",
    "\n",
    "    energy_cur_frame=[]\n",
    "    for T in range(cur_frame.shape[1]): #x,y,z\n",
    "        T_sum = 0\n",
    "        for i in range(1,len(cur_frame)):\n",
    "            T_sum += math.pow(abs(dft_cur_frame[i,T]),2)\n",
    "\n",
    "        energy_cur_frame.append(T_sum / (len(cur_frame)-1))\n",
    "    energy_cur_frame = np.array(energy_cur_frame)\n",
    "    \n",
    "\n",
    "    std_cur_frame = []\n",
    "    for T in range(cur_frame.shape[1]): #x,y,z\n",
    "        std_cur_frame.append(np.std(cur_frame))\n",
    "    std_cur_frame = np.array(std_cur_frame)\n",
    "    \n",
    "    coorelation_cur_frame = []\n",
    "    for T1,T2 in [(0,1),(1,2),(0,2)]:\n",
    "        coorelation_cur_frame.append(np.correlate(cur_frame[:,T1], cur_frame[:,T2])[0])\n",
    "    coorelation_cur_frame = np.array(coorelation_cur_frame)\n",
    "    \n",
    "    return np.array([mean_cur_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shrink_array(array,size):\n",
    "    \n",
    "    ratio = float(len(array)) / float(size+1)\n",
    "    res = []\n",
    "    for i in range(size):\n",
    "        res.append(np.mean(array[math.floor(i*ratio):math.ceil((i+1.0)*ratio)], axis = 0))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for key in list(tag_id_dict.keys()):\n",
    "#     frame_feature = gesture_features(tag_id_groupby_acc.get_group(key)[['VALUES1', 'VALUES2', 'VALUES3']].values).reshape(-1)\n",
    "    acc_feature = shrink_array(tag_id_groupby_acc.get_group(key)[['VALUES1', 'VALUES2', 'VALUES3']].values, 30)\n",
    "#     acc_feature = min_max_normalization(acc_feature)\n",
    "    gyro_feature = shrink_array(tag_id_groupby_gyro.get_group(key)[['VALUES1', 'VALUES2', 'VALUES3']].values, 30)\n",
    "#     gyro_feature = min_max_normalization(gyro_feature)\n",
    "#     t = pd.to_datetime(tag_id_groupby_acc.get_group(key)['TIMESTAMP']).values\n",
    "#     time_dif = (np.max(t) - np.min(t)).item()/1000000000\n",
    "#     X.append(acc_feature)\n",
    "    X.append(np.concatenate((acc_feature, gyro_feature), axis = 1))\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = []\n",
    "for i in range(10):\n",
    "    tag_list.append(['Tag'+str(i),i])\n",
    "for i in tag_list:\n",
    "    tag_str = i[0]\n",
    "    tag_int = i[1]\n",
    "    y[y==tag_str] = tag_int\n",
    "y_categorical = to_categorical(y)\n",
    "idx = list(range(len(X)))\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx]\n",
    "y_categorical = y_categorical[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1070, 30, 6)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.70      0.68       107\n",
      "          1       0.88      0.86      0.87       107\n",
      "          2       0.84      0.90      0.87       107\n",
      "          3       0.89      0.84      0.87       107\n",
      "          4       0.87      0.83      0.85       107\n",
      "          5       0.85      0.82      0.83       107\n",
      "          6       0.76      0.73      0.74       107\n",
      "          7       0.86      0.85      0.85       107\n",
      "          8       0.84      0.86      0.85       107\n",
      "          9       0.79      0.81      0.80       107\n",
      "\n",
      "avg / total       0.82      0.82      0.82      1070\n",
      "\n",
      "[[75  1  1  1  2  2 16  2  5  2]\n",
      " [ 1 92  4  1  1  2  1  3  0  2]\n",
      " [ 2  0 96  4  1  0  0  3  0  1]\n",
      " [ 1  1  8 90  0  1  0  3  1  2]\n",
      " [ 3  1  1  0 89  2  1  0  3  7]\n",
      " [ 2  2  0  2  0 88  1  1  5  6]\n",
      " [20  1  0  1  4  1 78  0  2  0]\n",
      " [ 3  3  4  0  2  1  0 91  1  2]\n",
      " [ 5  0  0  0  0  5  4  0 92  1]\n",
      " [ 3  4  0  2  3  2  2  3  1 87]]\n",
      "[0.81308411214953269, 0.81308411214953269, 0.81308411214953269, 0.80841121495327106, 0.85514018691588789]\n",
      "0.820560747664\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "if cv > 1:\n",
    "    scores = []\n",
    "    tests = []\n",
    "    predicts = []\n",
    "    chunk = math.floor(len(X)/cv)\n",
    "    for i in range(1,1+cv):\n",
    "        test_idx = list(range((i-1)*chunk,i*chunk))\n",
    "        train_idx = [i for i in range(len(X)) if i not in test_idx]\n",
    "        train_x = X[train_idx].reshape(len(train_idx),-1)\n",
    "        train_y = y_categorical[train_idx]\n",
    "        test_x = X[test_idx].reshape(len(test_idx),-1)\n",
    "        test_y = y_categorical[test_idx]\n",
    "\n",
    "        # This returns a tensor\n",
    "        inputs = Input(shape=(train_x.shape[1:]))\n",
    "\n",
    "        # a layer instance is callable on a tensor, and returns a tensor\n",
    "        # con1 = Conv1D(filters=30,kernel_size=10)(inputs)\n",
    "        layer1 = Dense(64, activation='relu')(inputs)\n",
    "        layer2 = Dense(128, activation='relu')(layer1)\n",
    "        layer3 = Dense(64, activation='relu')(layer2)\n",
    "        layer4 = Dense(32, activation='relu')(layer3)\n",
    "        predictions = Dense(len(set(y)), activation='softmax')(layer4)\n",
    "\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "    #     print(model.summary())\n",
    "        model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        model_his = model.fit(train_x, train_y, batch_size=32, epochs=40, verbose = 0)  # starts training\n",
    "        pred_y = model.predict(test_x)\n",
    "\n",
    "        # print(classification_report(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "        # print(confusion_matrix(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "        scores.append(accuracy_score(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "        tests += np.argmax(test_y,1).tolist()\n",
    "        predicts += np.argmax(pred_y, 1).tolist()\n",
    "print(classification_report(tests, predicts))\n",
    "print(confusion_matrix(tests, predicts))\n",
    "print(scores)\n",
    "print(np.mean(np.array(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_71 (InputLayer)        (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 21, 30)            1830      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 21, 30)            1830      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_73 (InputLayer)        (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 30)            1830      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_74 (InputLayer)        (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 21, 30)            1830      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 21, 30)            1830      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.77      0.75       107\n",
      "          1       0.86      0.88      0.87       107\n",
      "          2       0.85      0.87      0.86       107\n",
      "          3       0.90      0.86      0.88       107\n",
      "          4       0.94      0.90      0.92       107\n",
      "          5       0.80      0.84      0.82       107\n",
      "          6       0.81      0.78      0.79       107\n",
      "          7       0.90      0.89      0.90       107\n",
      "          8       0.85      0.82      0.84       107\n",
      "          9       0.80      0.82      0.81       107\n",
      "\n",
      "avg / total       0.84      0.84      0.84      1070\n",
      "\n",
      "[[82  1  3  0  0  2 11  1  4  3]\n",
      " [ 0 94  2  0  0  2  1  4  1  3]\n",
      " [ 3  0 93  4  0  0  0  2  4  1]\n",
      " [ 0  3  5 92  1  3  1  0  1  1]\n",
      " [ 3  0  1  0 96  2  1  0  1  3]\n",
      " [ 1  2  1  3  0 90  2  0  3  5]\n",
      " [16  0  1  1  2  2 83  0  0  2]\n",
      " [ 1  4  3  0  0  1  0 95  1  2]\n",
      " [ 3  1  0  0  1  7  4  1 88  2]\n",
      " [ 4  4  1  2  2  4  0  2  0 88]]\n",
      "[0.82242990654205606, 0.85046728971962615, 0.86915887850467288, 0.82242990654205606, 0.84579439252336452]\n",
      "0.842056074766\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "if cv > 1:\n",
    "    scores = []\n",
    "    tests = []\n",
    "    predicts = []\n",
    "    chunk = math.floor(len(X)/cv)\n",
    "    for i in range(1,1+cv):\n",
    "        test_idx = list(range((i-1)*chunk,i*chunk))\n",
    "        train_idx = [i for i in range(len(X)) if i not in test_idx]\n",
    "        train_x = X[train_idx]\n",
    "        train_y = y_categorical[train_idx]\n",
    "        test_x = X[test_idx]\n",
    "        test_y = y_categorical[test_idx]# This returns a tensor\n",
    "        input_val1 = Input(shape=train_x.shape[1:])\n",
    "\n",
    "        con1 = Conv1D(filters=30,kernel_size=10)(input_val1)\n",
    "        max_pooling_1d_1 = MaxPooling1D(pool_size=2, strides=None, padding='valid')(con1)\n",
    "        # con2 = Conv1D(filters=30,kernel_size=10)(max_pooling_1d_1)\n",
    "        # max_pooling_1d_2 = MaxPooling1D(pool_size=3, strides=None, padding='valid')(con2)\n",
    "        flat_1 = Flatten()(max_pooling_1d_1)\n",
    "\n",
    "        # input_val2 = Input(shape=(200,1))\n",
    "\n",
    "        # con3 = Conv1D(filters=30,kernel_size=10)(input_val2)\n",
    "        # max_pooling_1d_3 = MaxPooling1D(pool_size=2, strides=None, padding='valid')(con3)\n",
    "        # con4 = Conv1D(filters=30,kernel_size=10)(max_pooling_1d_3)\n",
    "        # max_pooling_1d_4 = MaxPooling1D(pool_size=3, strides=None, padding='valid')(con4)\n",
    "        # flat_2 = Flatten()(max_pooling_1d_4)\n",
    "\n",
    "        # input_val3 = Input(shape=(200,1))\n",
    "        # con6 = Conv1D(filters=30,kernel_size=10)(input_val3)\n",
    "        # max_pooling_1d_5 = MaxPooling1D(pool_size=2, strides=None, padding='valid')(con5)\n",
    "        # con5 = Conv1D(filters=30,kernel_size=10)(max_pooling_1d_5)\n",
    "        # max_pooling_1d_6 = MaxPooling1D(pool_size=3, strides=None, padding='valid')(con6)\n",
    "        # flat_3 = Flatten()(max_pooling_1d_6)\n",
    "\n",
    "        # concat = concatenate([flat_1,flat_2, flat_3])\n",
    "        # layer1 = Dense(64, activation='relu')(inputs)\n",
    "        layer2 = Dense(128, activation='relu')(flat_1)\n",
    "        # layer3 = Dense(64, activation='relu')(layer2)\n",
    "        layer4 = Dense(32, activation='relu')(layer2)\n",
    "        predictions = Dense(y_categorical.shape[-1], activation='softmax')(layer4)\n",
    "\n",
    "        # model = Model(inputs=[input_val1,input_val2, input_val3], outputs=predictions)\n",
    "        model = Model(inputs = input_val1, outputs=predictions)\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(optimizer='rmsprop',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        # model_his = model.fit([train_x[:,:,[0]],train_x[:,:,[1]],train_x[:,:,[2]]], train_y, batch_size=32, epochs=40, verbose = 0)  # starts training\n",
    "        model_his = model.fit(train_x, train_y, batch_size=32, epochs=40, verbose = 0)  # starts training\n",
    "        # pred_y = model.predict([test_x[:,:,[0]],test_x[:,:,[1]],test_x[:,:,[2]]])\n",
    "        pred_y = model.predict(test_x)\n",
    "\n",
    "    #     print(classification_report(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "    #     print(confusion_matrix(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "        scores.append(accuracy_score(np.argmax(test_y,1), np.argmax(pred_y, 1)))\n",
    "        tests += np.argmax(test_y,1).tolist()\n",
    "        predicts += np.argmax(pred_y, 1).tolist()\n",
    "print(classification_report(tests, predicts))\n",
    "print(confusion_matrix(tests, predicts))\n",
    "print(scores)\n",
    "print(np.mean(np.array(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
